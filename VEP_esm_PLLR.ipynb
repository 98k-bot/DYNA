{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b8385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=1024, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=1)\n",
    "    per_device_eval_batch_size: int = field(default=1)\n",
    "    num_train_epochs: int = field(default=10)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=100)\n",
    "    save_steps: int = field(default=50)\n",
    "    eval_steps: int = field(default=50)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"eval_loss\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=False)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=50)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=1e-2)\n",
    "    learning_rate: float = field(default=1e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhangz2lab/zhanh/esm-variants/output_0831/output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement\n",
    "training_args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a785ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"bert-base-uncased\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"facebook/esm2_t6_8M_UR50D\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/esm1b_t33_650M_UR50S\")\n",
    "model_args = ModelArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.step_count = 0\n",
    "        self.alphabet = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "\n",
    "    def compute_pll_for_sequence(self, sequence, model):\n",
    "        #tokens = self.tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        tokens = self.tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        model_device = next(model.parameters()).device\n",
    "        for key in tokens.keys():\n",
    "            tokens[key] = tokens[key].to(model_device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model.base_model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "        \n",
    "        logits = torch.log_softmax(outputs.logits, dim=-1)\n",
    "        #print('logits',logits)\n",
    "        idx = [self.alphabet[t] for t in sequence]\n",
    "        PLL = torch.sum(torch.diag(logits[0, 1:-1, :][:, idx]))\n",
    "        return PLL.item()\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count == 1 or self.step_count % 50 == 0:  # You can adjust the frequency as needed.\n",
    "            all_sequences = []\n",
    "            df = pd.read_csv(\"/common/zhangz2lab/zhanh/esm-variants/cropped/cm_test_data_1024.csv\")\n",
    "            all_sequences = df['wt_seq'].tolist()\n",
    "\n",
    "            all_plls_wt = []\n",
    "            for seq in all_sequences:\n",
    "                wt_pll = self.compute_pll_for_sequence(seq, model)\n",
    "                all_plls_wt.append(wt_pll)\n",
    "\n",
    "            #print(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for all sequences: {all_plls_wt}\")\n",
    "            #logging.info(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for all sequences: {all_plls_wt}\")\n",
    "            \n",
    "            all_sequences = []\n",
    "            all_sequences = df['mut_seq'].tolist()\n",
    "\n",
    "            all_plls_mut = []\n",
    "            for seq in all_sequences:\n",
    "                mut_pll = self.compute_pll_for_sequence(seq, model)\n",
    "                all_plls_mut.append(mut_pll)\n",
    "\n",
    "            all_plls_wt = np.array(all_plls_wt)\n",
    "            all_plls_mut = np.array(all_plls_mut)\n",
    "        \n",
    "        # Compute the PLLR\n",
    "            PLLR_callback = np.abs(all_plls_wt - all_plls_mut)\n",
    "        \n",
    "        # Get true labels\n",
    "            true_labels_callback = df['labels'].to_numpy()\n",
    "\n",
    "        # Compute AUC and AUPR\n",
    "            fpr, tpr, _ = roc_curve(true_labels_callback, PLLR_callback)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aupr = average_precision_score(true_labels_callback, PLLR_callback)\n",
    "        \n",
    "        # Logging\n",
    "            #print(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for wt sequences: {all_plls_wt}\")\n",
    "            #print(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for mut sequences: {all_plls_mut}\")\n",
    "            logging.info(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for wt sequences: {all_plls_wt}\")\n",
    "            logging.info(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for mut sequences: {all_plls_mut}\")\n",
    "            print(f\"AUC: {roc_auc}\")\n",
    "            print(f\"Area Under the Precision-Recall Curve (AUPR): {aupr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        self.tokenizer = tokenizer\n",
    "        # Generating some random sequences for demonstration purposes\n",
    "        #self.seq_a = [\"AGTCCGTA\" * 10 for _ in range(num_examples)]\n",
    "        #self.seq_b = [\"TCGATCGA\" * 10 for _ in range(num_examples)]\n",
    "        #self.labels = [np.random.randint(0,2) for _ in range(num_examples)]  # Random binary labels\n",
    "        self.seq_a = list(data['wt_seq'])\n",
    "        self.seq_b = list(data['mut_seq'])\n",
    "        self.labels = list(data['labels'])\n",
    "        self.num_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs_a = self.tokenizer(self.seq_a[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        inputs_b = self.tokenizer(self.seq_b[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids1\": inputs_a[\"input_ids\"].squeeze(0), \n",
    "            \"attention_mask1\": inputs_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids2\": inputs_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask2\": inputs_b[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "#         inputs_a = self.tokenizer(self.seq_a[idx], max_length=training_args.model_max_length)\n",
    "#         inputs_b = self.tokenizer(self.seq_b[idx], max_length=training_args.model_max_length)\n",
    "        \n",
    "#         input_ids1 = torch.tensor(inputs_a[\"input_ids\"]).squeeze(0) if isinstance(inputs_a[\"input_ids\"], list) else inputs_a[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask1 = torch.tensor(inputs_a[\"attention_mask\"]).squeeze(0) if isinstance(inputs_a[\"attention_mask\"], list) else inputs_a[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "#         input_ids2 = torch.tensor(inputs_b[\"input_ids\"]).squeeze(0) if isinstance(inputs_b[\"input_ids\"], list) else inputs_b[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask2 = torch.tensor(inputs_b[\"attention_mask\"]).squeeze(0) if isinstance(inputs_b[\"attention_mask\"], list) else inputs_b[\"attention_mask\"].squeeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_labels, cache_dir=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Load the base model\n",
    "        self.base_model = transformers.AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            cache_dir=cache_dir,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        #self.classifier = nn.Sequential(\n",
    "        #    nn.Linear(self.base_model.config.hidden_size * 2, 128),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(128, num_labels)\n",
    "        #)\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels):\n",
    "        # Encoding sequences using the same base model\n",
    "        #output1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        #output2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "        #print(self.base_model)\n",
    "        outputs1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        #print(\"Shape of logits1:\", outputs1.logits.shape)\n",
    "        output1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "\n",
    "        outputs2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        #print(\"Shape of logits2:\", outputs2.logits.shape)\n",
    "        output2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "\n",
    "        #print(\"Shape of [CLS] 1:\", output1.shape)\n",
    "        #print(\"Shape of [CLS] 2:\", output2.shape)\n",
    "        \n",
    "        logits1 = torch.log_softmax(outputs1.logits, dim=-1)\n",
    "        logits2 = torch.log_softmax(outputs2.logits, dim=-1)\n",
    "        #print('logits1',logits1)\n",
    "        #print('logits2',logits2)\n",
    "        batch_size = input_ids1.shape[0]\n",
    "        \n",
    "        \n",
    "        PLLs1 = torch.zeros(batch_size, device=input_ids1.device)\n",
    "        PLLs2 = torch.zeros(batch_size, device=input_ids2.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            idx1 = input_ids1[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "            PLLs1[i] = torch.sum(torch.diag(logits1[i, 1:-1, :][:, idx1]))\n",
    "        for i in range(batch_size):\n",
    "            idx2 = input_ids2[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "            PLLs2[i] = torch.sum(torch.diag(logits2[i, 1:-1, :][:, idx2]))\n",
    "        PLLR = torch.abs(PLLs1 - PLLs2) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Concatenating the outputs or a distance can be computed\n",
    "        #combined_out = torch.cat((output1, output2), dim=1)\n",
    "        # Classifying the combined outputs\n",
    "        #logits = self.classifier(combined_out)\n",
    "        #loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        #return logits, loss\n",
    "        # Compute cosine similarity between output1 and output2\n",
    "        cosine_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "        \n",
    "        target = torch.where(labels == 1, -torch.ones_like(cosine_sim), torch.ones_like(cosine_sim)).float()\n",
    "        \n",
    "        # Compute the loss between cosine similarity and the labels\n",
    "        # Assuming labels are in the range [-1, 1], denoting similarity\n",
    "        loss = F.mse_loss(cosine_sim, target)\n",
    "        #print(\"Shape of PLLs:\", PLLs1.shape)\n",
    "        #print(\"Shape of cosine_sim:\", cosine_sim.shape)\n",
    "        #print(\"Shape of target:\", target.shape)\n",
    "        sigmoid_PLLR = torch.sigmoid(PLLR)\n",
    "        pll_loss = F.binary_cross_entropy(sigmoid_PLLR, labels.float())\n",
    "        \n",
    "        #return (loss, cosine_sim)\n",
    "        return (pll_loss, PLLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c5dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class: <class 'transformers.models.esm.tokenization_esm.EsmTokenizer'>\n",
      "Tokenizer name: EsmTokenizer\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(model_args.model_name_or_path, num_labels=2)\n",
    "data_path = \"/common/zhangz2lab/zhanh/esm-variants/cropped/\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, \n",
    "                                                       model_max_length=training_args.model_max_length,) \n",
    "                                                       #padding_side=\"right\",\n",
    "                                                       #use_fast=True,\n",
    "                                                       #trust_remote_code=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#         model_name_or_path,\n",
    "#         model_max_length=512,\n",
    "#         padding_side=\"right\",\n",
    "#         use_fast=True,\n",
    "#         trust_remote_code=True,\n",
    "#     )\n",
    "print(\"Tokenizer class:\", tokenizer.__class__)\n",
    "print(\"Tokenizer name:\", tokenizer.__class__.__name__)\n",
    "    \n",
    "#train_dataset = SiameseDataset(tokenizer, 900)\n",
    "#test_dataset = SiameseDataset(tokenizer, 100)\n",
    "train_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_train_data_1024.csv'))\n",
    "test_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_test_data_1024.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee81360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiple sequences\n",
    "# sequence = \"MKLWTA\"\n",
    "# encoded_sequence = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence)\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoded_sequence)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Multiple sequences\n",
    "# sequences = [\"MKLWTA\", \"GATCRY\"]\n",
    "# encoded_sequences = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "\n",
    "# # Output as lists of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequences['input_ids'])\n",
    "\n",
    "# # Convert token IDs back to tokens for each sequence\n",
    "# for i, ids in enumerate(encoded_sequences['input_ids']):\n",
    "#     print(f\"Tokens for sequence {i+1}: {tokenizer.convert_ids_to_tokens(ids)}\")\n",
    "\n",
    "\n",
    "\n",
    "# input_df = pd.read_csv('/common/zhanh/Cardioboost_protein_sequences/cm_train_protein_seq_df.csv')\n",
    "# print(input_df.loc[0, 'Original_Protein_Sequence'])\n",
    "# encoded_sequence1 = tokenizer.encode(input_df.loc[0, 'Original_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence1,len(encoded_sequence1))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens1 = tokenizer.convert_ids_to_tokens(encoded_sequence1)\n",
    "# print(\"Tokens:\", tokens1)\n",
    "\n",
    "# print(input_df.loc[0, 'Mutated_Protein_Sequence'])\n",
    "# encoded_sequence2 = tokenizer.encode(input_df.loc[0, 'Mutated_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence2,len(encoded_sequence2))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens2 = tokenizer.convert_ids_to_tokens(encoded_sequence2)\n",
    "# print(\"Tokens:\", tokens2)\n",
    "\n",
    "# differences = []\n",
    "# for i, (char1, char2) in enumerate(zip(encoded_sequence1, encoded_sequence2)):\n",
    "#     if char1 != char2:\n",
    "#         differences.append((i, char1, char2))\n",
    "\n",
    "# print(f\"Differences found at these positions: {differences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a0e379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    cosine_sims, labels = eval_pred\n",
    "    mse = ((cosine_sims - labels)**2).mean()\n",
    "    # Flip the sign of the cosine similarities because we want -1 for label 1 and 1 for label 0\n",
    "    flipped_cosine_sims = -cosine_sims\n",
    "    \n",
    "    # Convert these flipped values to \"probabilities\" in [0, 1]\n",
    "    probabilities = (flipped_cosine_sims + 1) / 2  # Now values are between 0 and 1\n",
    "\n",
    "    # Make binary predictions based on a threshold (e.g., 0.7)\n",
    "    predictions = (probabilities > 0.1).astype(np.int32)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    auc = roc_auc_score(labels, probabilities)\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ea8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_PLLR(eval_pred):\n",
    "    PLLR, labels = eval_pred\n",
    "    auc = roc_auc_score(labels, PLLR)\n",
    "    aupr = average_precision_score(labels, PLLR)\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'aupr':aupr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics for evaluation\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {\n",
    "#         'accuracy': (predictions == labels).mean()\n",
    "#     }\n",
    "\n",
    "def custom_data_collator(data):\n",
    "    # Here, we ensure that each item in `data` has the necessary keys.\n",
    "    input_ids1 = torch.stack([item['input_ids1'] for item in data])\n",
    "    attention_mask1 = torch.stack([item['attention_mask1'] for item in data])\n",
    "    input_ids2 = torch.stack([item['input_ids2'] for item in data])\n",
    "    attention_mask2 = torch.stack([item['attention_mask2'] for item in data])\n",
    "\n",
    "    # Ensure labels exist or handle its absence\n",
    "    #labels = [item.get('labels', torch.tensor(-1)) for item in data]  # Using -1 as a default\n",
    "    #labels = torch.stack(labels)\n",
    "    labels = torch.stack([item['labels'] for item in data])\n",
    "\n",
    "    return {\n",
    "        'input_ids1': input_ids1,\n",
    "        'attention_mask1': attention_mask1,\n",
    "        'input_ids2': input_ids2,\n",
    "        'attention_mask2': attention_mask2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "custom_callback_instance = CustomCallback(tokenizer=tokenizer)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_PLLR,\n",
    "    data_collator=custom_data_collator,\n",
    "    callbacks=[custom_callback_instance]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6bed2a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [0, 4, 5, 6, 7, 9, 10, 2]\n",
      "Token IDs: [0, 32, 32, 32, 32, 32, 32, 2]\n",
      "Output1 shape: torch.Size([1, 1024, 33])\n",
      "Output2 shape: torch.Size([1, 1024, 33])\n"
     ]
    }
   ],
   "source": [
    "# Let's assume you've already loaded your trained model into a variable named `model`.\n",
    "\n",
    "# Create tokens for your test sequences using your tokenizer.\n",
    "# Assuming `tokenizer` is your tokenizer and `test_seq1` and `test_seq2` are your test sequences.\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move model to device\n",
    "model.to(device)\n",
    "\n",
    "test_seq1 = \"LAGVER\"\n",
    "test_seq2 = \"<mask><mask><mask><mask><mask><mask>\"\n",
    "encoded_sequence = tokenizer.encode(test_seq1, add_special_tokens=True)\n",
    "\n",
    "# # # Output as a list of token IDs\n",
    "print(\"Token IDs:\", encoded_sequence)\n",
    "encoded_sequence = tokenizer.encode(test_seq2, add_special_tokens=True)\n",
    "\n",
    "# # # Output as a list of token IDs\n",
    "print(\"Token IDs:\", encoded_sequence)\n",
    "tokens1 = tokenizer(test_seq1, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "tokens2 = tokenizer(test_seq2, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)  # Assuming test_seq2 contains mask tokens.\n",
    "\n",
    "label = torch.tensor([1], dtype=torch.int64).to(device)\n",
    "\n",
    "# # Move tensors to device\n",
    "tokens1 = {k: v.to(device) for k, v in tokens1.items()}\n",
    "tokens2 = {k: v.to(device) for k, v in tokens2.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "     outputs1 = model.base_model(input_ids=tokens1['input_ids'], attention_mask=tokens1['attention_mask'])\n",
    "     output1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "\n",
    "     outputs2 = model.base_model(input_ids=tokens2['input_ids'], attention_mask=tokens2['attention_mask'])\n",
    "     output2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "\n",
    "# # Examine the outputs\n",
    "print(\"Output1 shape:\", outputs1.logits.shape)\n",
    "print(\"Output2 shape:\", outputs2.logits.shape)\n",
    "\n",
    "logits = torch.log_softmax(outputs1.logits, dim=-1)\n",
    "s = logits[0][1:-1,:].shape\n",
    "alphabet = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "idx = [alphabet[t] for t in test_seq1]\n",
    "PLL = torch.sum(torch.diag(logits[0, 1:-1, :][:, idx]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718f3f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 03:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.507319450378418, 'eval_auc': 0.7080508474576271, 'eval_aupr': 0.6913082977487603, 'eval_runtime': 42.4979, 'eval_samples_per_second': 5.13, 'eval_steps_per_second': 2.565}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8114406779661016\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.8376820587982237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 451/2200 23:43 < 1:32:25, 0.32 it/s, Epoch 2.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Aupr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>17.394800</td>\n",
       "      <td>10.402319</td>\n",
       "      <td>0.726441</td>\n",
       "      <td>0.701769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>11.409100</td>\n",
       "      <td>10.412347</td>\n",
       "      <td>0.720593</td>\n",
       "      <td>0.696346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>13.600800</td>\n",
       "      <td>12.032263</td>\n",
       "      <td>0.725085</td>\n",
       "      <td>0.704576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>17.398300</td>\n",
       "      <td>13.254223</td>\n",
       "      <td>0.718475</td>\n",
       "      <td>0.684111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>15.613600</td>\n",
       "      <td>14.088445</td>\n",
       "      <td>0.707712</td>\n",
       "      <td>0.679195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>13.452800</td>\n",
       "      <td>14.439485</td>\n",
       "      <td>0.714068</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>10.162400</td>\n",
       "      <td>8.406562</td>\n",
       "      <td>0.745339</td>\n",
       "      <td>0.677659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>10.669500</td>\n",
       "      <td>7.986630</td>\n",
       "      <td>0.737542</td>\n",
       "      <td>0.675175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 67/109 00:24 < 00:15, 2.67 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8257627118644068\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.849923272065294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8416949152542372\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.8637113195404343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8673728813559323\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.8910326176714258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8794915254237288\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.9022537180542193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8741525423728815\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.9005507041605496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8753389830508475\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.9014742510742556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8770338983050848\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.9059885092139073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8672881355932204\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.8943964489872661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8733898305084746\n",
      "Area Under the Precision-Recall Curve (AUPR): 0.8983705072372893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2006\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2287\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2285\u001b[0m             metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2287\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2993\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2990\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2992\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2993\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3003\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:3174\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3171\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3173\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3174\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3175\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:3391\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3388\u001b[0m     return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_return_loss\n\u001b[1;32m   3389\u001b[0m loss_without_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m return_loss \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3391\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2637\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]:\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;124;03m    Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;124;03m    handling potential state.\u001b[39;00m\n\u001b[1;32m   2636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2637\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2640\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2641\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2642\u001b[0m         )\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2619\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2616\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m-> 2619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2619\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2616\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m-> 2619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[0;32m/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/transformers/trainer.py:2629\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mis_floating_point(data) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(data)):\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;66;03m# NLP models inputs are int/uint and those get adjusted to the right dtype of the\u001b[39;00m\n\u001b[1;32m   2626\u001b[0m         \u001b[38;5;66;03m# embedding. Other models such as wav2vec2's inputs are already float and thus\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m         \u001b[38;5;66;03m# may need special handling to match the dtypes of the model\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhf_deepspeed_config\u001b[38;5;241m.\u001b[39mdtype()})\n\u001b[0;32m-> 2629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761ef66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA LORA",
   "language": "python",
   "name": "dna_lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
