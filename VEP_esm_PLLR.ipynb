{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b8385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=1024, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=8)\n",
    "    num_train_epochs: int = field(default=5)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=100)\n",
    "    save_steps: int = field(default=50)\n",
    "    eval_steps: int = field(default=50)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"eval_loss\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=False)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=50)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=1e-2)\n",
    "    learning_rate: float = field(default=1e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhangz2lab/zhanh/esm-variants/output_0831/output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement\n",
    "training_args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a785ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"bert-base-uncased\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"facebook/esm2_t6_8M_UR50D\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/esm2_t12_35M_UR50D\")\n",
    "model_args = ModelArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.step_count = 0\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def compute_pll_for_all_sequences(self, all_sequences, model, tokenizer):\n",
    "        all_plls = []\n",
    "        for sequence in all_sequences:\n",
    "            print(sequence)\n",
    "            tokens = tokenizer([sequence], return_tensors='pt', padding=True, truncation=True)\n",
    "            model_device = next(model.parameters()).device\n",
    "            for key in tokens.keys():\n",
    "                tokens[key] = tokens[key].to(model_device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "        \n",
    "            logits = torch.log_softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            s = logits[0][1:-1,:].shape\n",
    "            alphabet = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "            idx = [alphabet[t] for t in sequence]\n",
    "            PLL = np.sum(np.diag(logits[0][1:-1, :][:, idx]))\n",
    "            all_plls.append(PLL)\n",
    "            all_plls_array = np.array(all_plls)\n",
    "            #np.savetxt(\"/common/zhangz2lab/zhanh/esm-variants/cropped/all_plls_mut.csv\", all_plls_array, delimiter=',', header='PLL_mut', comments='')\n",
    "            #np.savetxt(\"/common/zhangz2lab/zhanh/esm-variants/cropped/all_plls.csv\", all_plls_array, delimiter=',', header='PLL_wt', comments='')\n",
    "        return np.array(all_plls)\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count == 1 or self.step_count % 10 == 0:  # adjust the frequency as you need\n",
    "            all_sequences = []\n",
    "            df = pd.read_csv(\"/common/zhangz2lab/zhanh/esm-variants/cropped/cm_test_data_1024.csv\")\n",
    "            wt_seq_list = df['wt_seq'].tolist()\n",
    "            all_sequences = wt_seq_list\n",
    "            #all_sequences = [\"AAAA\",\"AAAC\"]\n",
    "            all_plls = self.compute_pll_for_all_sequences(all_sequences, model, tokenizer)\n",
    "            print(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for all sequences: {all_plls}\")\n",
    "            logging.info(f\"Step {self.step_count}: Pseudo-Log-Likelihoods for all sequences: {all_plls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        self.tokenizer = tokenizer\n",
    "        # Generating some random sequences for demonstration purposes\n",
    "        #self.seq_a = [\"AGTCCGTA\" * 10 for _ in range(num_examples)]\n",
    "        #self.seq_b = [\"TCGATCGA\" * 10 for _ in range(num_examples)]\n",
    "        #self.labels = [np.random.randint(0,2) for _ in range(num_examples)]  # Random binary labels\n",
    "        self.seq_a = list(data['wt_seq'])\n",
    "        self.seq_b = list(data['mut_seq'])\n",
    "        self.labels = list(data['labels'])\n",
    "        self.num_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs_a = self.tokenizer(self.seq_a[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        inputs_b = self.tokenizer(self.seq_b[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids1\": inputs_a[\"input_ids\"].squeeze(0), \n",
    "            \"attention_mask1\": inputs_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids2\": inputs_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask2\": inputs_b[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "#         inputs_a = self.tokenizer(self.seq_a[idx], max_length=training_args.model_max_length)\n",
    "#         inputs_b = self.tokenizer(self.seq_b[idx], max_length=training_args.model_max_length)\n",
    "        \n",
    "#         input_ids1 = torch.tensor(inputs_a[\"input_ids\"]).squeeze(0) if isinstance(inputs_a[\"input_ids\"], list) else inputs_a[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask1 = torch.tensor(inputs_a[\"attention_mask\"]).squeeze(0) if isinstance(inputs_a[\"attention_mask\"], list) else inputs_a[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "#         input_ids2 = torch.tensor(inputs_b[\"input_ids\"]).squeeze(0) if isinstance(inputs_b[\"input_ids\"], list) else inputs_b[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask2 = torch.tensor(inputs_b[\"attention_mask\"]).squeeze(0) if isinstance(inputs_b[\"attention_mask\"], list) else inputs_b[\"attention_mask\"].squeeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_labels, cache_dir=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Load the base model\n",
    "        self.base_model = transformers.AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            cache_dir=cache_dir,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.base_model.config.hidden_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels):\n",
    "        # Encoding sequences using the same base model\n",
    "        #output1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        #output2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "        #print(self.base_model)\n",
    "        outputs1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        #print(\"Shape of logits1:\", outputs1.logits.shape)\n",
    "        output1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "\n",
    "        outputs2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        #print(\"Shape of logits2:\", outputs2.logits.shape)\n",
    "        output2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "\n",
    "        #print(\"Shape of [CLS] 1:\", output1.shape)\n",
    "        #print(\"Shape of [CLS] 2:\", output2.shape)\n",
    "        \n",
    "        logits1 = torch.log_softmax(outputs1.logits, dim=-1)\n",
    "        logits2 = torch.log_softmax(outputs2.logits, dim=-1)\n",
    "        \n",
    "        batch_size = input_ids1.shape[0]\n",
    "        \n",
    "        PLLs1 = torch.zeros(batch_size, device=input_ids1.device)\n",
    "        PLLs2 = torch.zeros(batch_size, device=input_ids2.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            idx1 = input_ids1[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "            PLLs1[i] = torch.sum(torch.diag(logits1[i, 1:-1, :][:, idx1]))\n",
    "        for i in range(batch_size):\n",
    "            idx2 = input_ids2[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "            PLLs2[i] = torch.sum(torch.diag(logits2[i, 1:-1, :][:, idx2]))\n",
    "        PLLR = PLLs1 - PLLs2 \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Concatenating the outputs or a distance can be computed\n",
    "        #combined_out = torch.cat((output1, output2), dim=1)\n",
    "        # Classifying the combined outputs\n",
    "        #logits = self.classifier(combined_out)\n",
    "        #loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        #return logits, loss\n",
    "        # Compute cosine similarity between output1 and output2\n",
    "        cosine_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "        \n",
    "        target = torch.where(labels == 1, -torch.ones_like(cosine_sim), torch.ones_like(cosine_sim)).float()\n",
    "        \n",
    "        # Compute the loss between cosine similarity and the labels\n",
    "        # Assuming labels are in the range [-1, 1], denoting similarity\n",
    "        loss = F.mse_loss(cosine_sim, target)\n",
    "        #print(\"Shape of PLLs:\", PLLs1.shape)\n",
    "        #print(\"Shape of cosine_sim:\", cosine_sim.shape)\n",
    "        #print(\"Shape of target:\", target.shape)\n",
    "        sigmoid_PLLR = torch.sigmoid(PLLR)\n",
    "        pll_loss = F.binary_cross_entropy(sigmoid_PLLR, labels.float())\n",
    "        \n",
    "        #return (loss, cosine_sim)\n",
    "        return (pll_loss, PLLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c5dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class: <class 'transformers.models.esm.tokenization_esm.EsmTokenizer'>\n",
      "Tokenizer name: EsmTokenizer\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(model_args.model_name_or_path, num_labels=2)\n",
    "data_path = \"/common/zhangz2lab/zhanh/esm-variants/cropped/\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, \n",
    "                                                       model_max_length=training_args.model_max_length,) \n",
    "                                                       #padding_side=\"right\",\n",
    "                                                       #use_fast=True,\n",
    "                                                       #trust_remote_code=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#         model_name_or_path,\n",
    "#         model_max_length=512,\n",
    "#         padding_side=\"right\",\n",
    "#         use_fast=True,\n",
    "#         trust_remote_code=True,\n",
    "#     )\n",
    "print(\"Tokenizer class:\", tokenizer.__class__)\n",
    "print(\"Tokenizer name:\", tokenizer.__class__.__name__)\n",
    "    \n",
    "#train_dataset = SiameseDataset(tokenizer, 900)\n",
    "#test_dataset = SiameseDataset(tokenizer, 100)\n",
    "train_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_train_data_1024.csv'))\n",
    "test_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_train_data_1024.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee81360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiple sequences\n",
    "# sequence = \"MKLWTA\"\n",
    "# encoded_sequence = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence)\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoded_sequence)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Multiple sequences\n",
    "# sequences = [\"MKLWTA\", \"GATCRY\"]\n",
    "# encoded_sequences = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "\n",
    "# # Output as lists of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequences['input_ids'])\n",
    "\n",
    "# # Convert token IDs back to tokens for each sequence\n",
    "# for i, ids in enumerate(encoded_sequences['input_ids']):\n",
    "#     print(f\"Tokens for sequence {i+1}: {tokenizer.convert_ids_to_tokens(ids)}\")\n",
    "\n",
    "\n",
    "\n",
    "# input_df = pd.read_csv('/common/zhanh/Cardioboost_protein_sequences/cm_train_protein_seq_df.csv')\n",
    "# print(input_df.loc[0, 'Original_Protein_Sequence'])\n",
    "# encoded_sequence1 = tokenizer.encode(input_df.loc[0, 'Original_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence1,len(encoded_sequence1))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens1 = tokenizer.convert_ids_to_tokens(encoded_sequence1)\n",
    "# print(\"Tokens:\", tokens1)\n",
    "\n",
    "# print(input_df.loc[0, 'Mutated_Protein_Sequence'])\n",
    "# encoded_sequence2 = tokenizer.encode(input_df.loc[0, 'Mutated_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence2,len(encoded_sequence2))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens2 = tokenizer.convert_ids_to_tokens(encoded_sequence2)\n",
    "# print(\"Tokens:\", tokens2)\n",
    "\n",
    "# differences = []\n",
    "# for i, (char1, char2) in enumerate(zip(encoded_sequence1, encoded_sequence2)):\n",
    "#     if char1 != char2:\n",
    "#         differences.append((i, char1, char2))\n",
    "\n",
    "# print(f\"Differences found at these positions: {differences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a0e379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    cosine_sims, labels = eval_pred\n",
    "    mse = ((cosine_sims - labels)**2).mean()\n",
    "    # Flip the sign of the cosine similarities because we want -1 for label 1 and 1 for label 0\n",
    "    flipped_cosine_sims = -cosine_sims\n",
    "    \n",
    "    # Convert these flipped values to \"probabilities\" in [0, 1]\n",
    "    probabilities = (flipped_cosine_sims + 1) / 2  # Now values are between 0 and 1\n",
    "\n",
    "    # Make binary predictions based on a threshold (e.g., 0.7)\n",
    "    predictions = (probabilities > 0.1).astype(np.int32)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    auc = roc_auc_score(labels, probabilities)\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3dfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_PLLR(eval_pred):\n",
    "    PLLR, labels = eval_pred\n",
    "    auc = roc_auc_score(labels, PLLR)\n",
    "    return {\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics for evaluation\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {\n",
    "#         'accuracy': (predictions == labels).mean()\n",
    "#     }\n",
    "\n",
    "def custom_data_collator(data):\n",
    "    # Here, we ensure that each item in `data` has the necessary keys.\n",
    "    input_ids1 = torch.stack([item['input_ids1'] for item in data])\n",
    "    attention_mask1 = torch.stack([item['attention_mask1'] for item in data])\n",
    "    input_ids2 = torch.stack([item['input_ids2'] for item in data])\n",
    "    attention_mask2 = torch.stack([item['attention_mask2'] for item in data])\n",
    "\n",
    "    # Ensure labels exist or handle its absence\n",
    "    #labels = [item.get('labels', torch.tensor(-1)) for item in data]  # Using -1 as a default\n",
    "    #labels = torch.stack(labels)\n",
    "    labels = torch.stack([item['labels'] for item in data])\n",
    "\n",
    "    return {\n",
    "        'input_ids1': input_ids1,\n",
    "        'attention_mask1': attention_mask1,\n",
    "        'input_ids2': input_ids2,\n",
    "        'attention_mask2': attention_mask2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "custom_callback_instance = CustomCallback(tokenizer=tokenizer)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_PLLR,\n",
    "    data_collator=custom_data_collator,\n",
    "    #callbacks=[custom_callback_instance]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6bed2a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [0, 4, 5, 6, 7, 9, 10, 2]\n",
      "Token IDs: [0, 32, 32, 32, 32, 32, 32, 2]\n",
      "Output1 shape: torch.Size([1, 1024, 33])\n",
      "Output2 shape: torch.Size([1, 1024, 33])\n"
     ]
    }
   ],
   "source": [
    "# Let's assume you've already loaded your trained model into a variable named `model`.\n",
    "\n",
    "# Create tokens for your test sequences using your tokenizer.\n",
    "# Assuming `tokenizer` is your tokenizer and `test_seq1` and `test_seq2` are your test sequences.\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move model to device\n",
    "model.to(device)\n",
    "\n",
    "test_seq1 = \"LAGVER\"\n",
    "test_seq2 = \"<mask><mask><mask><mask><mask><mask>\"\n",
    "encoded_sequence = tokenizer.encode(test_seq1, add_special_tokens=True)\n",
    "\n",
    "# # # Output as a list of token IDs\n",
    "print(\"Token IDs:\", encoded_sequence)\n",
    "encoded_sequence = tokenizer.encode(test_seq2, add_special_tokens=True)\n",
    "\n",
    "# # # Output as a list of token IDs\n",
    "print(\"Token IDs:\", encoded_sequence)\n",
    "tokens1 = tokenizer(test_seq1, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "tokens2 = tokenizer(test_seq2, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)  # Assuming test_seq2 contains mask tokens.\n",
    "\n",
    "label = torch.tensor([1], dtype=torch.int64).to(device)\n",
    "\n",
    "# # Move tensors to device\n",
    "tokens1 = {k: v.to(device) for k, v in tokens1.items()}\n",
    "tokens2 = {k: v.to(device) for k, v in tokens2.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "     outputs1 = model.base_model(input_ids=tokens1['input_ids'], attention_mask=tokens1['attention_mask'])\n",
    "     output1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "\n",
    "     outputs2 = model.base_model(input_ids=tokens2['input_ids'], attention_mask=tokens2['attention_mask'])\n",
    "     output2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "\n",
    "# # Examine the outputs\n",
    "print(\"Output1 shape:\", outputs1.logits.shape)\n",
    "print(\"Output2 shape:\", outputs2.logits.shape)\n",
    "\n",
    "logits = torch.log_softmax(outputs1.logits, dim=-1)\n",
    "s = logits[0][1:-1,:].shape\n",
    "alphabet = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "idx = [alphabet[t] for t in test_seq1]\n",
    "PLL = torch.sum(torch.diag(logits[0, 1:-1, :][:, idx]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718f3f78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 01:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.475100</td>\n",
       "      <td>2.712423</td>\n",
       "      <td>0.792194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.754400</td>\n",
       "      <td>2.345574</td>\n",
       "      <td>0.894771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.345573902130127, 'eval_auc': 0.8947707795989683, 'eval_runtime': 6.3269, 'eval_samples_per_second': 69.544, 'eval_steps_per_second': 4.426, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761ef66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA LORA",
   "language": "python",
   "name": "dna_lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
