{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c364732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b8385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=1024, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=8)\n",
    "    num_train_epochs: int = field(default=5)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=100)\n",
    "    save_steps: int = field(default=50)\n",
    "    eval_steps: int = field(default=50)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"eval_loss\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=False)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    logging_steps: int = field(default=50)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=1e-2)\n",
    "    learning_rate: float = field(default=1e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=10)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"/common/zhangz2lab/zhanh/esm-variants/output_0831/output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement\n",
    "training_args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a785ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"bert-base-uncased\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/esm2_t6_8M_UR50D\")\n",
    "model_args = ModelArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        self.tokenizer = tokenizer\n",
    "        # Generating some random sequences for demonstration purposes\n",
    "        #self.seq_a = [\"AGTCCGTA\" * 10 for _ in range(num_examples)]\n",
    "        #self.seq_b = [\"TCGATCGA\" * 10 for _ in range(num_examples)]\n",
    "        #self.labels = [np.random.randint(0,2) for _ in range(num_examples)]  # Random binary labels\n",
    "        self.seq_a = list(data['wt_seq'])\n",
    "        self.seq_b = list(data['mut_seq'])\n",
    "        self.labels = list(data['labels'])\n",
    "        self.num_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs_a = self.tokenizer(self.seq_a[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        inputs_b = self.tokenizer(self.seq_b[idx], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids1\": inputs_a[\"input_ids\"].squeeze(0), \n",
    "            \"attention_mask1\": inputs_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids2\": inputs_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask2\": inputs_b[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "#         inputs_a = self.tokenizer(self.seq_a[idx], max_length=training_args.model_max_length)\n",
    "#         inputs_b = self.tokenizer(self.seq_b[idx], max_length=training_args.model_max_length)\n",
    "        \n",
    "#         input_ids1 = torch.tensor(inputs_a[\"input_ids\"]).squeeze(0) if isinstance(inputs_a[\"input_ids\"], list) else inputs_a[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask1 = torch.tensor(inputs_a[\"attention_mask\"]).squeeze(0) if isinstance(inputs_a[\"attention_mask\"], list) else inputs_a[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "#         input_ids2 = torch.tensor(inputs_b[\"input_ids\"]).squeeze(0) if isinstance(inputs_b[\"input_ids\"], list) else inputs_b[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask2 = torch.tensor(inputs_b[\"attention_mask\"]).squeeze(0) if isinstance(inputs_b[\"attention_mask\"], list) else inputs_b[\"attention_mask\"].squeeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_labels, cache_dir=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Load the base model\n",
    "        self.base_model = transformers.AutoModel.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.base_model.config.hidden_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels):\n",
    "        # Encoding sequences using the same base model\n",
    "        output1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        output2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Concatenating the outputs or a distance can be computed\n",
    "        combined_out = torch.cat((output1, output2), dim=1)\n",
    "        \n",
    "        # Classifying the combined outputs\n",
    "        #logits = self.classifier(combined_out)\n",
    "        #loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        #return logits, loss\n",
    "        # Compute cosine similarity between output1 and output2\n",
    "        cosine_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "        \n",
    "        target = torch.where(labels == 1, -torch.ones_like(cosine_sim), torch.ones_like(cosine_sim)).float()\n",
    "        \n",
    "        # Compute the loss between cosine similarity and the labels\n",
    "        # Assuming labels are in the range [-1, 1], denoting similarity\n",
    "        loss = F.mse_loss(cosine_sim, target)\n",
    "        \n",
    "        return (loss, cosine_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c5dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class: <class 'transformers.models.esm.tokenization_esm.EsmTokenizer'>\n",
      "Tokenizer name: EsmTokenizer\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(model_args.model_name_or_path, num_labels=2)\n",
    "data_path = \"/common/zhangz2lab/zhanh/esm-variants/cropped/\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, \n",
    "                                                       model_max_length=training_args.model_max_length,) \n",
    "                                                       #padding_side=\"right\",\n",
    "                                                       #use_fast=True,\n",
    "                                                       #trust_remote_code=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#         model_name_or_path,\n",
    "#         model_max_length=512,\n",
    "#         padding_side=\"right\",\n",
    "#         use_fast=True,\n",
    "#         trust_remote_code=True,\n",
    "#     )\n",
    "print(\"Tokenizer class:\", tokenizer.__class__)\n",
    "print(\"Tokenizer name:\", tokenizer.__class__.__name__)\n",
    "    \n",
    "#train_dataset = SiameseDataset(tokenizer, 900)\n",
    "#test_dataset = SiameseDataset(tokenizer, 100)\n",
    "train_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_train_data_1024.csv'))\n",
    "test_dataset = SiameseDataset(tokenizer, os.path.join(data_path, 'cm_train_data_1024.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee81360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiple sequences\n",
    "# sequence = \"MKLWTA\"\n",
    "# encoded_sequence = tokenizer.encode(sequence, add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence)\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoded_sequence)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Multiple sequences\n",
    "# sequences = [\"MKLWTA\", \"GATCRY\"]\n",
    "# encoded_sequences = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "\n",
    "# # Output as lists of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequences['input_ids'])\n",
    "\n",
    "# # Convert token IDs back to tokens for each sequence\n",
    "# for i, ids in enumerate(encoded_sequences['input_ids']):\n",
    "#     print(f\"Tokens for sequence {i+1}: {tokenizer.convert_ids_to_tokens(ids)}\")\n",
    "\n",
    "\n",
    "\n",
    "# input_df = pd.read_csv('/common/zhanh/Cardioboost_protein_sequences/cm_train_protein_seq_df.csv')\n",
    "# print(input_df.loc[0, 'Original_Protein_Sequence'])\n",
    "# encoded_sequence1 = tokenizer.encode(input_df.loc[0, 'Original_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence1,len(encoded_sequence1))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens1 = tokenizer.convert_ids_to_tokens(encoded_sequence1)\n",
    "# print(\"Tokens:\", tokens1)\n",
    "\n",
    "# print(input_df.loc[0, 'Mutated_Protein_Sequence'])\n",
    "# encoded_sequence2 = tokenizer.encode(input_df.loc[0, 'Mutated_Protein_Sequence'], add_special_tokens=True)\n",
    "\n",
    "# # Output as a list of token IDs\n",
    "# print(\"Token IDs:\", encoded_sequence2,len(encoded_sequence2))\n",
    "\n",
    "# # Convert token IDs back to tokens\n",
    "# tokens2 = tokenizer.convert_ids_to_tokens(encoded_sequence2)\n",
    "# print(\"Tokens:\", tokens2)\n",
    "\n",
    "# differences = []\n",
    "# for i, (char1, char2) in enumerate(zip(encoded_sequence1, encoded_sequence2)):\n",
    "#     if char1 != char2:\n",
    "#         differences.append((i, char1, char2))\n",
    "\n",
    "# print(f\"Differences found at these positions: {differences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a0e379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    cosine_sims, labels = eval_pred\n",
    "    mse = ((cosine_sims - labels)**2).mean()\n",
    "    # Flip the sign of the cosine similarities because we want -1 for label 1 and 1 for label 0\n",
    "    flipped_cosine_sims = -cosine_sims\n",
    "    \n",
    "    # Convert these flipped values to \"probabilities\" in [0, 1]\n",
    "    probabilities = (flipped_cosine_sims + 1) / 2  # Now values are between 0 and 1\n",
    "\n",
    "    # Make binary predictions based on a threshold (e.g., 0.7)\n",
    "    predictions = (probabilities > 0.1).astype(np.int32)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    auc = roc_auc_score(labels, probabilities)\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics for evaluation\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {\n",
    "#         'accuracy': (predictions == labels).mean()\n",
    "#     }\n",
    "\n",
    "def custom_data_collator(data):\n",
    "    # Here, we ensure that each item in `data` has the necessary keys.\n",
    "    input_ids1 = torch.stack([item['input_ids1'] for item in data])\n",
    "    attention_mask1 = torch.stack([item['attention_mask1'] for item in data])\n",
    "    input_ids2 = torch.stack([item['input_ids2'] for item in data])\n",
    "    attention_mask2 = torch.stack([item['attention_mask2'] for item in data])\n",
    "\n",
    "    # Ensure labels exist or handle its absence\n",
    "    #labels = [item.get('labels', torch.tensor(-1)) for item in data]  # Using -1 as a default\n",
    "    #labels = torch.stack(labels)\n",
    "    labels = torch.stack([item['labels'] for item in data])\n",
    "\n",
    "    return {\n",
    "        'input_ids1': input_ids1,\n",
    "        'attention_mask1': attention_mask1,\n",
    "        'input_ids2': input_ids2,\n",
    "        'attention_mask2': attention_mask2,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718f3f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.198600</td>\n",
       "      <td>2.161961</td>\n",
       "      <td>0.458772</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.106500</td>\n",
       "      <td>2.155076</td>\n",
       "      <td>0.458741</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.600310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/common/zhanh/anaconda3/envs/dna_lora/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.155075788497925, 'eval_mse': 0.45874145199578426, 'eval_accuracy': 0.4636363636363636, 'eval_f1': 0.016666666666666666, 'eval_precision': 1.0, 'eval_recall': 0.008403361344537815, 'eval_auc': 0.6003099259505782, 'eval_runtime': 2.6239, 'eval_samples_per_second': 167.687, 'eval_steps_per_second': 10.671, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22c58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA LORA",
   "language": "python",
   "name": "dna_lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
